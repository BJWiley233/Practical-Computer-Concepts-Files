{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> Brian Wiley <br/>\n",
    "EN.705.601.3VL.SP20 Applied Machine Learning </div>\n",
    "\n",
    "## Assignment 13\n",
    "### Applied Machine Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. [20 pts] Pre-process a single file using 20 frequency bins (i.e. M), and 2000 sampling\n",
    "frequency (controls the number of data points), such as,__\n",
    "\n",
    "`x, fs = librosa.load(Path_dataset+'cat_1.wav', sr=SAMPLING_FRQ)\n",
    "mfccs = librosa.feature.mfcc(x, sr=fs, n_mfcc=M)\n",
    "librosa.display.specshow(mfccs, sr=fs, x_axis='time')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bjwil\\Anaconda3\\envs\\torch\\lib\\site-packages\\librosa\\util\\decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\n",
      "  from numba.decorators import jit as optional_jit\n",
      "C:\\Users\\bjwil\\Anaconda3\\envs\\torch\\lib\\site-packages\\librosa\\util\\decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\n",
      "  from numba.decorators import jit as optional_jit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x16a093a9a20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "Path_dataset = 'C:/Users/bjwil/JHU Classes/Machine Learning JHU/cats_dogs/'\n",
    "SAMPLING_FRQ = 2000\n",
    "BINS = 20\n",
    "\n",
    "x, fs = librosa.load(Path_dataset+'cat_1.wav', sr=SAMPLING_FRQ)\n",
    "mfccs = librosa.feature.mfcc(x, sr=fs, n_mfcc=BINS)\n",
    "librosa.display.specshow(mfccs, sr=fs, x_axis='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. [20 pts] For each wav file, you will have multiple data points, as generated by the `librosa.feature.mfcc`. Generate the X and y matrices for supervised learning. Apply your favorite classifier and comment about your results. (Hint: Expect 80-90% 10-fold CV accuracy, and `N, M = 7634, 20`)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Cat files=164, # Dog files=113\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "files = [i for i in glob.glob(Path_dataset + '*.wav')]\n",
    "\n",
    "r = re.compile('.*cats_dogs\\\\\\\\cat_*')\n",
    "cat_files =  list(filter(r.match, files))\n",
    "\n",
    "r = re.compile('.*cats_dogs\\\\\\\\dog_*')\n",
    "dog_files =  list(filter(r.match, files))\n",
    "\n",
    "print('# Cat files={}, # Dog files={}'.format(len(cat_files), len(dog_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7634, 20)\n",
      "(7634, 1)\n"
     ]
    }
   ],
   "source": [
    "X = np.empty((0, 20))\n",
    "y = np.empty((0,1))\n",
    "\n",
    "## cat = 0 for target\n",
    "for i in range(len(cat_files)):\n",
    "    x, fs = librosa.load(cat_files[i], sr=SAMPLING_FRQ)\n",
    "    mfccs = librosa.feature.mfcc(x, sr=fs, n_mfcc=BINS)\n",
    "    X = np.append(mfccs.T, X, axis=0)\n",
    "    y = np.append(np.repeat(0, mfccs.T.shape[0]).reshape(-1,1), y, axis=0)\n",
    "\n",
    "## dog = 1 for target   \n",
    "for i in range(len(dog_files)):\n",
    "    x, fs = librosa.load(dog_files[i], sr=SAMPLING_FRQ)\n",
    "    mfccs = librosa.feature.mfcc(x, sr=fs, n_mfcc=BINS)\n",
    "    X = np.append(mfccs.T, X, axis=0)\n",
    "    y = np.append(np.repeat(1, mfccs.T.shape[0]).reshape(-1,1), y, axis=0)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bjwil\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'classifier__C': 10.0, 'classifier__kernel': 'rbf', 'classifier__tol': 1e-06}\n",
      "Best score: 0.9510086455331412\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate, GridSearchCV\n",
    "\n",
    "## first find best parameters\n",
    "kfold = StratifiedKFold(n_splits=2, shuffle=True)\n",
    "\n",
    "SVC_Pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', SVC())\n",
    "    ])\n",
    "\n",
    "C = np.logspace(-4, 1, 6)\n",
    "param_grid = dict(classifier__C=C,\n",
    "                  classifier__kernel=['rbf', 'linear', 'poly'],\n",
    "                  classifier__tol=np.logspace(-6,-2, 5))\n",
    "\n",
    "grid_SVC = GridSearchCV(SVC_Pipeline, param_grid=param_grid,\n",
    "                           cv=kfold, n_jobs=4, verbose=0,\n",
    "                           scoring='accuracy')\n",
    "\n",
    "grid_SVC.fit(X, y)\n",
    "print('Best params:', grid_SVC.best_params_)\n",
    "print('Best score:', grid_SVC.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC 10-fold cross validation test_score is 0.968 Â±0.0079\n"
     ]
    }
   ],
   "source": [
    "## 10-fold cross validation\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "SVC_Pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', SVC(C=10.0, kernel='rbf', tol=1e-06))\n",
    "    ])\n",
    "\n",
    "SVC_cross_validate = cross_validate(SVC_Pipeline, X, y.ravel(), cv=kfold, scoring='accuracy')\n",
    "\n",
    "for k, v in SVC_cross_validate.items():\n",
    "    if (k != 'fit_time' and k != 'score_time'):\n",
    "        print(f'SVC 10-fold cross validation {k} is {np.mean(v):.3f} {chr(177)}{np.std(v):.4f}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. [50 pts] Train a simple RNN, as given in the module Jupyter notebook, by shuffling the list\n",
    "of the signals (the dataset has 277) and training the network sufficiently (suggested 50\n",
    "times) and compare its performance to your previous evaluation in step (2.). A prediction\n",
    "function is needed, such as,__\n",
    " \n",
    " `with torch.no_grad():\n",
    "     hidden = rnn.init_hidden()\n",
    "     sxx = torch.tensor(sxx, dtype=torch.float)\n",
    "     for i in range(sxx.shape[0]):\n",
    "     output, hidden = rnn.forward(sxx[i].reshape(1,rnn.n_features), hidden)\n",
    "         y = 0 if ct=='cat' else 1\n",
    "     y_pred = 0 if output[0][0]>output[0][1] else 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, n_features, n_hidden, n_output, n_epochs, eta=0.0005):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_epochs = n_epochs\n",
    "        \n",
    "        self.i2h = nn.Linear(n_features + n_hidden, n_hidden).float()\n",
    "        self.i2o  = nn.Linear(n_features + n_hidden, n_output).float()\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        self.eta = eta\n",
    "        self.criterion = nn.NLLLoss()\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        combined = torch.cat((x, hidden), dim=1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros((1, self.n_hidden), dtype=torch.float)\n",
    "       \n",
    "    def train1(self, sxx, y):\n",
    "        '''\n",
    "        Trains 1 signal at a time\n",
    "        '''\n",
    "        self.zero_grad()\n",
    "        hidden = self.init_hidden()\n",
    "        N = sxx.shape[0]\n",
    "                        \n",
    "        for i in range(N):\n",
    "            output, hidden = self.forward(sxx[i].reshape(1, self.n_features), hidden)\n",
    "        \n",
    "        loss = self.criterion(output, y)\n",
    "        loss.backward()\n",
    "        \n",
    "        for p in self.parameters():\n",
    "            p.data.add_(-self.eta, p.grad.data)\n",
    "        \n",
    "        return output, loss.item()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        for epoch in range(self.n_epochs):\n",
    "            ## shuffle the indices\n",
    "            indices = [i for i in range(len(X))]\n",
    "            np.random.shuffle(indices)\n",
    "                \n",
    "            for i in indices:\n",
    "                xi = torch.tensor(X[i], dtype=torch.float)\n",
    "                yi = torch.tensor([y[i]], dtype=torch.long)\n",
    "                output, loss = self.train1(xi, yi)\n",
    "                ## for debugging\n",
    "                top_n, top_i = output.topk(1)\n",
    "                cat = top_i.item()\n",
    "                #print(\"Train guessed: {}, actually was {}\".format(cat, yi.numpy()[0]))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for signal in X:\n",
    "                hidden = self.init_hidden()\n",
    "                xi = torch.tensor(signal, dtype=torch.float)\n",
    "                ## this does same as loop in train\n",
    "                for i in range(xi.shape[0]):\n",
    "                    output, hidden = self.forward(xi[i].reshape(1, self.n_features), hidden)\n",
    "                ## gets prediction by top probability\n",
    "                ## see topk under https://pytorch.org/docs/stable/torch.html#comparison-ops\n",
    "                top_n, top_i = output.topk(1)\n",
    "                cat = top_i.item()\n",
    "                predictions.append(cat)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277\n",
      "277\n"
     ]
    }
   ],
   "source": [
    "## input lists for X and y\n",
    "## We can use lists because we are just going to convert single signals and targets to tensors anyway\n",
    "\n",
    "X_rnn = []\n",
    "y_rnn = []\n",
    "\n",
    "## cat = 0 for target\n",
    "for i in range(len(cat_files)):\n",
    "    x, fs = librosa.load(cat_files[i], sr=SAMPLING_FRQ)\n",
    "    mfccs = librosa.feature.mfcc(x, sr=fs, n_mfcc=BINS)\n",
    "    X_rnn.append(mfccs.T)\n",
    "    y_rnn.append(0)\n",
    "\n",
    "## dog = 1 for target   \n",
    "for i in range(len(dog_files)):  \n",
    "    x, fs = librosa.load(dog_files[i], sr=SAMPLING_FRQ)\n",
    "    mfccs = librosa.feature.mfcc(x, sr=fs, n_mfcc=BINS)\n",
    "    X_rnn.append(mfccs.T)\n",
    "    y_rnn.append(1)\n",
    "    \n",
    "print(len(X_rnn))\n",
    "print(len(y_rnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 80/20 split = 0.7857142857142857\n"
     ]
    }
   ],
   "source": [
    "## train on 80% and test on 20%\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_rnn, y_rnn, train_size=.8)   \n",
    "\n",
    "## create RNN instance\n",
    "rnn = RNN(n_features=20, n_hidden=20, n_output=2, n_epochs=50, eta=.0005)  \n",
    "\n",
    "## fit\n",
    "rnn.fit(X_train, y_train)\n",
    "\n",
    "## predict and get accuracy\n",
    "y_pred = rnn.predict(X_test) \n",
    "print(\"Accuracy on 80/20 split =\", accuracy_score(y_test, y_pred))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was getting 85% accuracy.  Not sure why I am getting around 60-80% now.  I didn't change anything from running this Tuesday night."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4. [10 pts] Compare and contrast the method applied in this assignment to the image classification.__\n",
    "\n",
    "The RNN in this model is different to than the image classification is that at least for MNIST the images are all the same size.  For the tensorflow module in the text on page 443, however you can have different sized images.  In this model for the RNN the sizes of the audio file are different lengths are we reshaped so they would have at least the same amount of features.  You might be able to do this for different sized images but I don't think making them all have the same amount of features would work since then that would be the width for each image.  In the RNN we train 1 sample at a time at the end back propogate on all the time steps of that specific audio before upgrading the weights.  With the images classification in the MLP we train on batchs of samples, so not 1 at a time, and then back propogate the batch from all the outputs to the input of the batch and then update the weights with the learning rate and the delta weights calculated from the gradient."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
