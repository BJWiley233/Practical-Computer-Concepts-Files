{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> Brian Wiley <br/>\n",
    "EN.705.601.3VL.SP20 Applied Machine Learning </div>\n",
    "\n",
    "## Assignment 9\n",
    "### Applied Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. [10 pts] Describe the environment in the Nim learning model.__\n",
    "\n",
    "The environment is the set of states from which the Nim learner interacts with by taking objects from pile in those states.  Simply the environment is the game of Nim where players choose to select objects from a pile in a series of choices to win the game.  Included in this q-learning environment is also a q-table from which the q-learner uses as a quote unquote cheat sheet to determine from memory what choices they have been made in the past that were good or bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. [10 pts] Describe the agent in the Nim learning model.__\n",
    "\n",
    "As precluded above, the agent of learning model is a player, in our case the Nim learner player.  The agent is instructed what the goal of the game is which is to win the game.  In our model we don't actually tell the Nim learner directly that clearing the last pile is the goal we do this by adding rewards and penalties if it won or lost.  Which I will go into below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. [10 pts] Describe the reward and penalty in the Nim learning model.__\n",
    "\n",
    "The reward and penalties can only be distributed once the goal of the game is reached, i.e. we have a winner.  As indicated in our textbook, because during the game of Nim we cannot know when a particular move is bad or good.  Therefore the reward/penalty is distributed after all piles are empty.  The learner from lecture only distributed a reward in which that state and specific action at the state to win the game is rewarded.  I adapted to include a penalty which will be described in section 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4. [10 pts] How many possible states there could be in the Nim game with a maximum of 10\n",
    "items per pile and 3 piles total?__\n",
    "\n",
    "Each pile can have between 0 and 10 objects so the total states is $11^{3}$ or 1,331.  This from the permuation formula where we have and order but can have repetition, i.e. 1,8,1 and 8,1,1 is the same permutation with different order.  We use the power rule to come up with the total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5. [10 pts] How many possible actions there could be in the Nim game with 10 items per pile\n",
    "and 3 piles total?__\n",
    "\n",
    "For each of the three piles as each pile could contain 1 or 2 or 3 or 4 or 5 or 6 or 7 or 8 or 9 or 10 objects and the player can take as many objects from only of the piles there are 10 possible actions per pile or 30 total action for the Nim game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6. [50 pts] Find a way to improve the provided Nim game learning model. Do you think one\n",
    "can beat the Guru player? (Hint: How about penalizing the losses? Hint: It is indeed\n",
    "possible to find a better solution which improves the way Q-learning updates its Q-table).__\n",
    "\n",
    "I added a new function for nim_qlearn after the final qtable_log function at the bottom.  To sum up I found [resource](https://cdn.cs50.net/ai/2020/spring/projects/4/nim.zip) where in the train model for the q-learner, the learner is trained by playing games against itself.  We can keep track of player 0 and player 1 with a dictionary which keeps track of their last moves only.  When the game is one, we reward the state and action completed by the winner, i.e. the state before [0, 0, 0] and what pile and how many object chosen to win, and penalize the state and action that was completed last by the loser.  In actuality we would want to use graph theory here so that we could complete a DFS traverse in the reverse order possibly by using a stack or topological sorting as well to reward and penalize more than one state and action per winner and loser respectively.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint, choice\n",
    "\n",
    "# The number of piles is 3\n",
    "\n",
    "\n",
    "# max number of items per pile\n",
    "ITEMS_MX = 10\n",
    "\n",
    "# Initialize starting position\n",
    "def init_game():\n",
    "    return [randint(1,ITEMS_MX), randint(1,ITEMS_MX), randint(1,ITEMS_MX)]\n",
    "\n",
    "# Based on X-oring the item counts in piles - mathematical solution\n",
    "def nim_guru(st):\n",
    "    xored = st[0] ^ st[1] ^ st[2]\n",
    "    if xored == 0:\n",
    "        return nim_random(st)\n",
    "    #\n",
    "    for pile in range(3):\n",
    "        s = st[pile] ^ xored\n",
    "        if s <= st[pile]:\n",
    "            return st[pile]-s, pile\n",
    "\n",
    "# Random Nim player\n",
    "def nim_random(_st):\n",
    "    pile = choice([i for i in range(3) if _st[i]>0])  # find the non-empty piles\n",
    "    return randint(1, _st[pile]), pile  # random move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 10, 7]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[randint(1,ITEMS_MX), randint(1,ITEMS_MX), randint(1,ITEMS_MX)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nim_qlearner(_st):\n",
    "    # pick the best rewarding move\n",
    "    a = np.argmax(qtable[_st[0], _st[1], _st[2]])\n",
    "    # index is based on move, pile\n",
    "    move, pile = a%ITEMS_MX+1, a//ITEMS_MX\n",
    "    # check if qtable has generated a random but game illegal move - we have not explored there yet\n",
    "    if move <= 0 or _st[pile] < move :\n",
    "        move, pile = nim_random(_st)\n",
    "    #\n",
    "    return move, pile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Engines = {'Random':nim_random, 'Guru':nim_guru, 'Qlearner':nim_qlearner}\n",
    "\n",
    "def game(a, b):\n",
    "    state, side = init_game(), 'A'\n",
    "    while True:\n",
    "        engine = Engines[a] if side == 'A' else Engines[b]\n",
    "        move, pile = engine(state)\n",
    "        # print(state, move, pile)  # debug purposes\n",
    "        state[pile] -= move\n",
    "        if state == [0, 0, 0]:  # game ends\n",
    "            return side  # winning side\n",
    "        #\n",
    "        side = 'B' if side == 'A' else 'A'  # switch sides\n",
    "\n",
    "def play_games(_n, a, b):\n",
    "    from collections import defaultdict\n",
    "    wins = defaultdict(int)\n",
    "    for i in range(_n):\n",
    "        wins[game(a, b)] += 1\n",
    "    # info\n",
    "    print(f\"{_n} games, {a:>8s} {wins['A']}   {b:>8s} {wins['B']}\")\n",
    "    #\n",
    "    return wins['A'], wins['B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 games,   Random 471     Random 529\n",
      "1000 games,     Guru 996     Random 4\n",
      "1000 games,   Random 9       Guru 991\n",
      "1000 games,     Guru 945       Guru 55\n"
     ]
    }
   ],
   "source": [
    "# Play games\n",
    "play_games(1000, 'Random', 'Random')\n",
    "play_games(1000, 'Guru', 'Random')\n",
    "play_games(1000, 'Random', 'Guru')\n",
    "play_games(1000, 'Guru', 'Guru') ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtable, Alpha, Gamma, Reward = None, 1.0, 0.8, 100.0\n",
    "\n",
    "# learn from _n games, randomly played to explore the possible states\n",
    "def nim_qlearn(_n):\n",
    "    global qtable\n",
    "    # based on max items per pile\n",
    "    qtable = np.zeros((ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX*3), dtype=float)\n",
    "    # play _n games\n",
    "    for i in range(_n):\n",
    "        # first state is starting position\n",
    "        st1 = init_game()\n",
    "        while True:\n",
    "            # make a random move - exploration\n",
    "            move, pile = nim_random(st1)\n",
    "            st2 = list(st1)\n",
    "            # make the move\n",
    "            st2[pile] -= move\n",
    "            if st2 == [0, 0, 0]:  # game ends\n",
    "                qtable_update(Reward, st1, move, pile, 0)\n",
    "                break  # new game\n",
    "            #\n",
    "            qtable_update(0, st1, move, pile, np.max(qtable[st2[0], st2[1], st2[2]]))\n",
    "            st1 = st2\n",
    "\n",
    "# Equation 3 - update the qtable\n",
    "def qtable_update(r, _st1, move, pile, q_future_best):\n",
    "    a = pile*ITEMS_MX+move-1\n",
    "    qtable[_st1[0], _st1[1], _st1[2], a] = Alpha * (r + Gamma * q_future_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nim_qlearn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 games, Qlearner 703     Random 297\n",
      "1000 games,   Random 313   Qlearner 687\n"
     ]
    }
   ],
   "source": [
    "# Play games\n",
    "play_games(1000, 'Qlearner', 'Random')\n",
    "play_games(1000, 'Random', 'Qlearner') ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 games, Qlearner 577     Random 423\n",
      "1000 games, Qlearner 440     Random 560\n",
      "1000 games, Qlearner 656     Random 344\n",
      "1000 games, Qlearner 730     Random 270\n",
      "1000 games, Qlearner 705     Random 295\n",
      "1000 games, Qlearner 726     Random 274\n",
      "1000 games, Qlearner 708     Random 292\n",
      "Wall time: 15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# See the training size effect\n",
    "n_train = (3, 10, 100, 1000, 10000, 50000, 100000)\n",
    "wins = []\n",
    "for n in n_train:\n",
    "    nim_qlearn(n)\n",
    "    a, b = play_games(1000, 'Qlearner', 'Random')\n",
    "    wins += [a/(a+b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.577, 0.44, 0.656, 0.73, 0.705, 0.726, 0.708]\n"
     ]
    }
   ],
   "source": [
    "print(wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qtable_log(_fn):\n",
    "    with open(_fn, 'w') as fout:\n",
    "        s = 'state'\n",
    "        for a in range(ITEMS_MX*3):\n",
    "            move, pile = a%ITEMS_MX+1, a//ITEMS_MX\n",
    "            s += ',%02d_%01d' % (move,pile)\n",
    "        #\n",
    "        print(s, file=fout)\n",
    "        for i, j, k in [(i,j,k) for i in range(ITEMS_MX+1) for j in range(ITEMS_MX+1) for k in range(ITEMS_MX+1)]:\n",
    "            s = '%02d_%02d_%02d' % (i,j,k)\n",
    "            for a in range(ITEMS_MX*3):\n",
    "                r = qtable[i, j, k, a]\n",
    "                s += ',%.1f' % r\n",
    "            #\n",
    "            print(s, file=fout)\n",
    "#\n",
    "qtable_log('qtable_debug.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is the updated function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtable, Alpha, Gamma, Reward, Penalty = None, 1.0, 0.8, 100.0, -1000.0\n",
    "\n",
    "# learn from _n games, randomly played to explore the possible states\n",
    "def nim_qlearn_updated(_n):\n",
    "    '''\n",
    "    Nim q-learner is trained while playing games against inself\n",
    "    Updates are adding last moves made by winner and loser and penalyzing last move by loser\n",
    "    '''\n",
    "    global qtable\n",
    "    # based on max items per pile\n",
    "    qtable = np.zeros((ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX*3), dtype=float)\n",
    "    # play _n games\n",
    "    for i in range(_n):\n",
    "        # first state is starting position\n",
    "        st1 = init_game()\n",
    "        \n",
    "        ## this keeps track of the last move made by winner and loser\n",
    "        last = {\n",
    "            0: {\"st\": None, \"pile\": None, \"move\": None},\n",
    "            1: {\"st\": None, \"pile\": None, \"move\": None}\n",
    "        }\n",
    "        ## used to switch q-learner players\n",
    "        q_learner_player_num = 0\n",
    "        \n",
    "        while True:\n",
    "            # make a random move - exploration\n",
    "            move, pile = nim_random(st1)\n",
    "            st2 = list(st1)\n",
    "            # make the move\n",
    "            st2[pile] -= move\n",
    "            '''\n",
    "            Switch player, we do this before checking if there is a winner\n",
    "            so we get can the state, pile, and move from that was made last\n",
    "            by the loser by penalizing last[q_learner_player_num] states\n",
    "            and action.\n",
    "            \n",
    "            Example if player #0 removed the last object from the final pile\n",
    "            then that state st1, move and pile from above gets rewarded. Then\n",
    "            we switch the player number to #1 so we can get that for the penalty\n",
    "            \n",
    "            See Harvard's extension here https://cdn.cs50.net/ai/2020/spring/projects/4/nim.zip\n",
    "            Note they have the goal switched and clearing last pile loses the game\n",
    "            '''\n",
    "            last[q_learner_player_num][\"st\"] = st1     ## current player's last state\n",
    "            last[q_learner_player_num][\"pile\"] = pile\n",
    "            last[q_learner_player_num][\"move\"] = move\n",
    "            q_learner_player_num = 1 if q_learner_player_num == 0 else 0  ## switch player\n",
    "    \n",
    "            if st2 == [0, 0, 0]:  # game ends\n",
    "                ## the current, i.e. st1, pile, move that was presnt and made by the winner\n",
    "                qtable_update(Reward, st1, move, pile, 0)\n",
    "                ## penalize the last move and action of player that lost\n",
    "                ## i.e. the state and move before the state and move that won\n",
    "                qtable_update(Penalty, last[q_learner_player_num][\"st\"],\n",
    "                              last[q_learner_player_num][\"move\"],\n",
    "                              last[q_learner_player_num][\"pile\"], 0)\n",
    "                break  # new game\n",
    "            # this should be an `else` to see visually it updates only if no winner\n",
    "            else:\n",
    "                qtable_update(0, st1, move, pile, np.max(qtable[st2[0], st2[1], st2[2]]))\n",
    "            # update state\n",
    "            st1 = st2\n",
    "            \n",
    "\n",
    "# Equation 3 - update the qtable\n",
    "def qtable_update(r, _st1, move, pile, q_future_best):\n",
    "    a = pile*ITEMS_MX+move-1\n",
    "    qtable[_st1[0], _st1[1], _st1[2], a] = Alpha * (r + Gamma * q_future_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's test to see if we get better at least better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 games, Qlearner 22       Guru 978\n",
      "1000 games, Qlearner 25       Guru 975\n",
      "1000 games, Qlearner 33       Guru 967\n",
      "1000 games, Qlearner 36       Guru 964\n",
      "1000 games, Qlearner 26       Guru 974\n",
      "1000 games, Qlearner 22       Guru 978\n",
      "1000 games, Qlearner 20       Guru 980\n",
      "1000 games, Qlearner 18       Guru 982\n",
      "1000 games, Qlearner 22       Guru 978\n",
      "1000 games, Qlearner 18       Guru 982\n",
      "Wall time: 32min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "no_penalty_wins = []\n",
    "with_penalty_wins = []\n",
    "\n",
    "## original no penalty train\n",
    "nim_qlearn(10000000)\n",
    "## test is 5 times for 1000 games each\n",
    "for _ in range(5):\n",
    "    no_penalty_wins.append(play_games(1000, 'Qlearner', 'Guru')[0])\n",
    "\n",
    "## updated with penalty training\n",
    "nim_qlearn_updated(10000000)\n",
    "## test is 5 times for 1000 games each\n",
    "for _ in range(5):\n",
    "    with_penalty_wins.append(play_games(1000, 'Qlearner', 'Guru')[0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Penality          With Penality  \n",
      "      22                       22               \n",
      "      25                       20               \n",
      "      33                       18               \n",
      "      36                       22               \n",
      "      26                       18               \n"
     ]
    }
   ],
   "source": [
    "## print results\n",
    "print(f'{\"Without Penality\":<25} {\"With Penality\":<15}')\n",
    "for i in range(len(with_penalty_wins)):\n",
    "    print(f'{no_penalty_wins[i]:^15} {with_penalty_wins[i]:^32}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am not sure why but penalize the second to last move and state before the win actually makes the results worse.  I guess there is a better way to penalize but I can't think of one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
